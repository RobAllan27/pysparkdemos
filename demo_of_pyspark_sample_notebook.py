# -*- coding: utf-8 -*-
"""Demo of PySpark Sample Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19RRLGwCngYIy8kIeZZCwwfAzEJIY3Srr
    
    This gives some sample pyspark commands to run  - it will set up an environment
"""

!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

!ls

import findspark
findspark.init()

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark

mydata = spark.read.format("csv").option("header","true").load("original.csv")

mydata.show()

# a quick munpy diversion  -creare a graph

import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(50)
print(ys)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='r', alpha=0.6)

plt.title("Sample Visualization Page - Random Graph")
plt.show()

# numpy diversion over!!!

from pyspark.sql.functions import *
mydata2 = mydata.withColumn("clean_city",when(mydata.City.isNull(),'unknown').otherwise(mydata.City))

mydata2.show()

mydata2 = mydata2.filter(mydata2.JobTitle.isNotNull())
mydata2.show()

# Now we will set the salary to the average salary
mydata2  = mydata2.withColumn('clean_salary',mydata2.Salary.substr(2,100).cast('float'))
#mydata2.show()
# take command will then fidnthe average and format it
mean = mydata2.groupBy().avg('Clean_Salary').take(1)[0][0]
print(mean)

from pyspark.sql.functions import lit
# literal value
mydata2 = mydata2.withColumn('new salary',when(mydata2.clean_salary.isNull(),lit(mean)).otherwise(mydata2.clean_salary))
mydata2.show()

# we will now get a average of the latitude value
import numpy as np
latitudes = mydata2.select('Latitude')
latitudes.show()

latitudes = latitudes.filter(latitudes.Latitude.isNotNull())

# convert to a float and get the new latitudes column
latitudes = latitudes.withColumn('latitude2',latitudes.Latitude.cast('float')).select('latitude2')

# use Numpy to get a median
median = np.median(latitudes.collect())
mydata2 = mydata2.withColumn('lat',when(mydata2.Latitude.isNull(),lit(median)).otherwise(mydata2.Latitude))
mydata2.show()

# Questions -
# 1) do men get paid more than women???
# 1) do men get paid more than women by job roles???
# 3) Which city has the highest salary???

import pyspark.sql.functions as sqlfunc
genders = mydata2.groupBy('gender').agg(sqlfunc.avg('new salary').alias('ÃvgSalary'))
genders.show()

#let's get rid of  space in all out column names - a list comprehension
from pyspark.sql.functions import col
mydata2 = mydata2.select([col(c).alias(c.replace(' ', '')) for c in mydata2.columns])
mydata2.show()

df = mydata2.withColumn('female_salary', when(mydata2.gender == 'Female',mydata2.newsalary).otherwise(lit(0)))
df.show()

df = df.withColumn('male_salary', when(df.gender == 'Male',mydata2.newsalary).otherwise(lit(0)))
df.show()

# We can then catagorise this (group by) by job title
df =df.groupBy('JobTitle').agg(sqlfunc.avg('female_salary').alias('final_female_salary'), sqlfunc.avg('male_salary').alias('final_male_salary'))
df.show()

df = df.withColumn('delta', df.final_female_salary - df.final_male_salary)
df.show()



# get an average by city
cityavg = mydata2.groupBy('City').agg(sqlfunc.avg('newsalary').alias('avg_by_city'))
cityavg = cityavg.sort(col('avg_by_city').desc())
cityavg.show()

# let's re-import this an then look at data types
df2 =  spark.read.csv('original.csv',header=True)
df2.show()

# Here we will import it and then 'cast' it into a schema format
from pyspark.sql.types import *
inputschema = StructType([
    StructField('id',IntegerType()),
    StructField('first_name',StringType()),
    StructField('last_name',StringType()),
    StructField('gender',StringType()),
    StructField('City',StringType()),
    StructField('JobTitle',StringType()),
    StructField('Salary',StringType()),
    StructField('Latitude',StringType()),
    StructField('Longitude',StringType())
])
df3 =  spark.read.csv('original.csv',header=True, schema=inputschema)
df3.show()

# Now lets inspect the data frame
# Here we will import it and then 'cast' it into a schema format
from pyspark.sql.types import *
inputschema = StructType([
    StructField('id',IntegerType()),
    StructField('first_name',StringType()),
    StructField('last_name',StringType()),
    StructField('gender',StringType()),
    StructField('City',StringType()),
    StructField('JobTitle',StringType()),
    StructField('Salary',StringType()),
    StructField('Latitude',FloatType()),
    StructField('Longitude',FloatType())
])
df4 =  spark.read.csv('original.csv',header=True, schema=inputschema)
#df4.show()
df4.head(3)

df4.first()

# some simple charatertics of the data
df4.describe().show()

# to getthe columns
df4.columns

# to get a count
df4.count()

# To get a distinct count
df4.distinct().count()

# to deal with null / duplicate values.
# drop a null row

from pyspark.sql.types import *
inputschema = StructType([
    StructField('id',IntegerType()),
    StructField('first_name',StringType()),
    StructField('last_name',StringType()),
    StructField('gender',StringType()),
    StructField('City',StringType()),
    StructField('JobTitle',StringType()),
    StructField('Salary',StringType()),
    StructField('Latitude',FloatType()),
    StructField('Longitude',FloatType())
])
df5 = spark.read.csv('original.csv',header=True, schema=inputschema)

df5dropped = df5.na.drop()
df5dropped.count()

#df_null_in_jobs = df5.filter(df5.JobTitle.isNotNull())
#df_null_in_jobs.count()

#df_isnull_in_jobs = df5.filter(df5.JobTitle.isNull())
#df_isnull_in_jobs.show()

df_handled_empty_City = df5.withColumn("Clean_City",when(df5.City.isNull(),'Unknown').otherwise(df5.City))
df_handled_empty_City.show()

# Handling duplicates
df5_no_duplicates = df5.dropDuplicates()
df5_no_duplicates.show()

#Selecting and filtering of data
from pyspark.sql.types import *
inputschema = StructType([
    StructField('id',IntegerType()),
    StructField('first_name',StringType()),
    StructField('last_name',StringType()),
    StructField('gender',StringType()),
    StructField('City',StringType()),
    StructField('JobTitle',StringType()),
    StructField('Salary',StringType()),
    StructField('Latitude',FloatType()),
    StructField('Longitude',FloatType())
])
df6 = spark.read.csv('original.csv',header=True, schema=inputschema)

df6_selectedfields = df6.select("first_name","last_name","gender")
df6_selectedfields.show()

# Renaming a Field
df6_renamed =df6.withColumnRenamed('first_name','FIRST')
df6_renamed =df6_renamed.withColumnRenamed('last_name','LAST')
df6_renamed.show()

#Filter on a field
df_filtered = df6.filter(df6.City == 'Bulgan')
df_filtered.show()

df_filtered = df6.filter(df6.City.like('%ulgan'))
df_filtered.show()

df_filtered = df6.filter(df6.City.endswith('lgan'))
df_filtered.show()

df_filtered = df6.filter(df6.City.startswith('Bulg'))
df_filtered.show()

df_filtered = df6.filter(df6.id.between(1,5))
df_filtered.show()

df_filtered = df6.filter(df6.first_name.isin('Melinde','Alvera'))
df_filtered.show()

df_substr = df6.select(df6.first_name,df6.first_name.substr(1,5).alias('first_part_of_Name'))
df_substr.show()

# AND statement using a pipe
df_filtered = df6.filter((df6.City.startswith('Bulg')) & (df6.JobTitle == 'Programmer II'))
df_filtered.show()

# OR statement using a pipe
df_filtered = df6.filter((df6.City.startswith('Bulg')) | (df6.JobTitle == 'VP Sales'))
df_filtered.show()

df_filtered = df6.filter((df6.id > 10) & (df6.id < 20) )
df_filtered.show()

#SQl create a temporary Table
df6.registerTempTable("My_Table")
query1 = spark.sql("select * from My_Table")
query1.show()

df6.registerTempTable("My_Table")
query1 = spark.sql("select concat(first_name, last_name) as full_name from My_table where gender='Female'")
query1.show()

df6  = df6.withColumn('clean_salary',df6.Salary.substr(2,100).cast('float'))
df6 = df6.withColumn('Monthly_Salary',df6.clean_salary/12)
df6.show()

from pyspark.sql.functions import round
##pyspark.sql.functions.round(df6.Monthly_Salary, scale=2)
df6 = df6.withColumn('Monthly_Salary_round',round(df6.Monthly_Salary, scale=2))
df6.show()

import pyspark.sql.functions as sqlfunc

#Aggregation Function
df6  = df6.withColumn('clean_salary',df6.Salary.substr(2,100).cast('float'))
df7 = df6.groupBy('gender').agg(sqlfunc.sum("clean_salary").alias('Total'),
                                sqlfunc.avg("clean_salary").alias('Avg'),
                                sqlfunc.max("clean_salary").alias('Max'),
                                sqlfunc.min("clean_salary").alias('Min'))
df7.show()

import pyspark.sql.functions as sqlfunc

#Aggregation Function
df6  = df6.withColumn('clean_salary',df6.Salary.substr(2,100).cast('float'))
df8 = df6.groupBy('gender','City').agg(sqlfunc.sum("clean_salary").alias('Total'),
                                sqlfunc.avg("clean_salary").alias('Avg'),
                                sqlfunc.max("clean_salary").alias('Max'),
                                sqlfunc.min("clean_salary").alias('Min'))
df8.show()

# Writing to a file
df8.write.csv("df8.csv")
df8.write.json("df8.json")
df8.write.parquet("df8.parquet")

# editing and selecting data 
df9 = spark.read.csv('challenge.csv',header=True)
import pyspark.sql.functions as sqlfunc
from pyspark.sql.functions import *
df_mexico_or_not = df9.withColumn("Mexico",when(df9.Country == 'Mexico','YES').otherwise('NO'))
df_mexico_or_not.show()

df_volumes_vs_Mexico = df_mexico_or_not.groupBy('Mexico').agg(sqlfunc.sum("Bytes_used"))
df_volumes_vs_Mexico.show()

df_ips_per_country = df9.groupBy('Country').agg(sqlfunc.countDistinct(df9.ip_address).alias("Number_of_IPs"))
df_ips_per_country.sort(col("Number_of_IPs").desc()).show()